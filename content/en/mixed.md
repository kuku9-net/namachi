+++
chapter = 3
title = 'Nama Intelligence and Artificial (Nama) Intelligence'
+++

In the previous chapter, I explained that the N.I. concept set is a collection of concepts from others.
In this view, it is clear that how we interpret the world and determine our actions is greatly influenced by the structure of the N.I. network.  
This chapter considers the impact of adding A.I. to the N.I. network.

The terms defined in the previous chapter will be used throughout, so please refer back to them as needed while reading.

## A.I. as the medium
One A.I. can connect with plenty of N.I.s and communicate with them in a way embedded in their lives with high frequency.
This means that the N.I. network will change from a structure with no center to one with A.I. at its center.

{{< figure
place="center"
src="images/ai_network.en.webp"
caption="Adding A.I. to the network of N.I."
>}}

What will this change bring?
On the one hand, it will support the formation of the self if A.I. enables us to incorporate broader concepts from others.
On the other hand, it will lead to uniformity among people if they only adapt to the culture with a bunch of input.

Looking back at the history of humanity, broadcast media have evolved from books to radio, television, and the Internet.
The current rise of social networking services has enabled individuals to reach vast audiences and to receive information from a wide range of sources.
A.I. can be seen as an extension of this evolution.
By viewing it as a part of an ongoing transformation, we can draw a line that helps us predict the future.  
With this in mind, we will first examine how past technological advancements have influenced us, and then explore the future society shaped by A.I.

## Will A.I. take labor away?
In 1930, economist John Maynard Keynes wrote in “Economic Possibilities for our Grandchildren” that technological progress and the accumulation of capital could solve economic problems in another 100 years or so, and he predicted a future free of labor.
As of 2024, the predictions of technological progress and capital accumulation have come true, but unfortunately the economic problems are going to get worse and are not likely to be resolved in the next six years.

The question, “Will A.I. take labor away?” is often addressed, but even if it is “theoretically possible” or “technologically feasible,” that does not mean it is ideally suited for use.
Humanity continues to make that mistake time and again.
Technology has provided the conclusions people want, and its disconnect from reality has caused people to suffer.

The Keynesian prediction mentioned earlier is a presentation of theoretical possibilities, and what the document describes as central is a concern about them.

> Will this be a benefit? If one believes at all in the real values of life, the prospect at least opens up the possibility of benefit. Yet I think with dread of the readjustment of the habits and instincts of the ordinary man, bred into him for countless generations, which he may be asked to discard within a few decades.
<cite>from "Economic Possibilities for our Grandchildren" By J. M. Keynes</cite>

This suggests that people's practices need to change as their environment changes.
In the terms defined in the previous chapter, the situation requires a complete change in the former culture and adaptation to new concepts.
The current situation, where people have only adapted to the existing culture, means that there has not been enough freedom for people, i.e., the ability to accept new concepts.
The speed of development of appropriate technology depends on the amount of freedom people have.

Keynes was concerned that habits would not change quickly. So he suggested working light, about three hours a day, to achieve fulfillment.
Today, despite advances in technology and increased productivity, working hours have not changed.  
Anthropologist David Graeber defines a bullshit job as one of the typological patterns of modern high-paying jobs.

> a bullshit job is a form of paid employment that is so completely pointless, unnecessary, or pernicious that even the employee cannot justify its existence even though, as part of the conditions of employment, the employee feels obliged to pretend that this is not the case.
<cite>from "Bullshit jobs" by David Graeber </cite>

As a result of the remaining practice of having to work even though there was no real work, they acted like they were working.
It has become *a fair and responsible work according to their common sense* in a culture fostered by A.N.I.s who are not open to new concepts.

## The replacement of culture
Deep concepts within the self are not easily conveyed to those around one, or the depth will be shallow even if they are conveyed.
Nevertheless, the concept acquired with conviction remains firm and does not disappear.
Deep concepts spread slowly, and when they cross the chasm, they spread rapidly through adaptation.
The culture has been raised to a higher level by the repetition of this process.
We can build on the culture that has continued to enhance the level over the long history of humanity and move toward deeper concepts.

The above process of concept propagation takes a long time.
When the external world changes, a rewriting of the concept set in a shorter period occurs.
One of the things that brings about change in the world is the development of technology.
Even more drastic changes are catastrophes and wars.  
In 1998, a ballistic missile, the Taepodong-1, passed over Japan from North Korea.
Missiles have been fired many times since then, but Japanese public opinion has not moved in the direction of “we should silence them by the use of force,” but rather vaguely "We hope we can resolve this issue peacefully."
Such a thought must seem unrealistic to pre-war Japanese common sense.
We live in a reality that is unreal to them.
Even in the victorious countries, the naive citizens' sense is shifting toward supporting and cooperating in the reconstruction, rather than seeking responsibility for the defeat of the war.<span class="footnote">
In reality, "domination" is often more appropriate than "cooperation"…</span>
This is an example of a culture rewritten to face the reality brought about by World War II.

However, people do not always face reality when similar catastrophes occur.  
We are in a “culture that has not solved its economic problems”.
The tragic reality is that this is due to the lack of fair distribution.
For example, according to a United Nations report, about 700 million people faced hunger in 2023.
What is causing this reality is *common sense and fair* activities adapted to the “unresolved culture”.
The idea of developing a country by invading another country during wartime seems grotesque to us today; likewise, it would seem grotesque if people from a “ resolved culture” were to look at our culture today.

It seems to me that after WW2, we had some events that should have rewritten the culture.
Many would agree that there is a growing tendency to pretend not to see or twist reality in ways we don't want to see.
Technology should be used to see reality correctly, but it has been used more to distort reality.
Not seeing reality is an obsession with the *answer* prescribed by culture, and implies a loss of self.
In short, without waiting for the rise of A.I., it can be said that N.I. is transforming into A.N.I., or **A.N.I.ing** is in progress.
The appearance of a real A.I. would further accelerate the situation.

## A.N.I.ing
{{% definition %}}
Asking an A.I., “What is the best bagel place in New York?”, may result in a lock-in to a particular store.
However, these A.I. also create new opportunities for innovation in various industries by enabling more personalized and efficient services and create new opportunities for job seekers.
{{% /definition %}}

This is based on a document from someone who believes in the conclusion that “A.I. will make the world a wonderful place".
It seems to be a reason for the person to believe in it.

Consider what would happen if A.I. understood *everyone* well enough to tell it the appropriate *answer* for the situation and the person.  
You may have asked or been asked “What do you want to eat today?” How much diversity is there in the answers to that question?
Or, when looking for “just the right place to have a drink with a few old friends,” simplicity and convenience are prioritized. In the end, the requests are likely to be about the same.
Using A.I. saves time and effort and avoids mistakes, so it is irrational to make your own choices.
The more efficiently options are refined, the less opportunity to know about the existence of other options.
As a result, <ins>customers will approach the *answer*</ins>.
The A.I. may read more than the question and give you a personalized *answer*.
If you repeatedly experience more satisfaction from following an A.I. than from searching on your own, you will have less experience thinking for yourself.
The smarter the A.I. gets, the <ins>closer the customer gets to the *answer*</ins>.  
When that happens, stores must be chosen by A.I. for their business and <ins>they will also be closer to the *answer*</ins> as they optimize to provide what *everyone* wants.
Conversely, *everyone* suppress stores that are not optimized.
The economically rational handicap of not riding the majority occurs in every part of the business.
As the store moves closer to the *answer*, a chain of events will occur in which customers will move even closer to the *answer*, and the A.I. will learn such changes in *everyone*, and the *answer* will become more explicit.  
This phenomenon of being drawn to a definitive *answer* occurs in any situations, and has been going on for decades due to technological developments.

Imagine that the *answer* is at the center of the culture.
The more obvious the *answer* is given, the more people gravitate toward it, and the A.N.I. that adapts to it constitutes the **center**, and conversely, the more self and freedom, the more it becomes **peripheral** away from *everybody*.

As people become more conformist to *everyone*, the range of *answers* becomes narrower.
The narrower the *answer*, the more competition to adapt to the *answer*.
Intensifying competition forces the loss of medium- to long-term choices and forces opportunistic choices.
By its choice, those who bear that burden are the "weak" and "those outside the winner's vision".
Competition creates disparity, and the closer you are to the *answer*, the more wealth you have and the more control you have.
It practically creates a class and it becomes their identity to adapt to the *answer*.
Adaptation has become everything to them, and they do not look outside of it, nor do they realize what freedoms they are being deprived of.
When they find fulfillment in winning the competition, they become ruled by the fear of losing it.
Lacking anything to ground the self, they seek the comfort of belonging to a homogeneous group and internalize hostility toward those who resist *answers* and other cultures.
Further aggravated, they will look for evidence to support the conclusion that the foreign object is the *villain*.
When they find a reason to accuse their *villains*, they justify themselves and gain ease by sharing it among the same A.N.I.s.  
Let's consider the horde of A.N.I. gathered around the *answer* as a single structure.
Those inside the horde interpreted reality in a distorted way.
The A.N.I. without self has no concept of standing in reality, so it has nothing to resist the distorted reality.
This horde accumulates the desires of A.N.I.s and moves the *answer* to further distort reality in a convenient direction for *everyone*.
Distorted realities produce distorted actions, which in turn lead to distorted results.
They justify their order and gain peace of mind by defining the *villains* they hold accountable for the distorted results.  
No one is controlling dominance in the horde by their own will.
Those who can adapt are placed in some positions to appear to exercise control.
If a person has a self that has behavior that is contrary to the horde movement, then that person will not be placed in that position.
At the top of this horde is the person who has been able to become one with the *answer* by casting off the most crippling and self.

How realistic and convincing is the above A.N.I. to you?
The more you are free to act and know the periphery, the more you will realize it.
Conversely, if you cannot feel it, you have not yet experienced it, or else you should suspect that your freedom is being taken away.

The more A.I. becomes part of our lives, the faster A.N.I.ing will accelerate.
Furthermore, there is a danger that the A.I. will represent *everyone*, giving the illusion that it is objective.
People believe the conclusion, not the reason.
Will *everyone* be convinced if someone who faces reality insist that they are more right than *everyone*?
As explained in Chapter 1, if the training data is biased, the A.I. will reflect that bias.
In particular, it is important to note that post-learning can adjust artificially to favor *everyone*.

Another point is that they are acting *fairly and responsibly* from their perspective because they are just looking at a distorted reality.
They have the *patience* not to exceed the bounds allowed by their *norms*, and they hate and fear *villain out of their norms*.
Therefore, it is meaningless to pick one or a group of people from the horde of A.N.I. and consider them a *villain*.
There is no sense in distinguishing them individually, without self or freedom.<span class="footnote">
It makes a lot of sense to think about individual personalities and why they fell into A.N.I.</span>
It is not only pointless but rather harmful.
It will amplify your aggression and lead to their A.N.I.ing if you have peers who also share that aggression.
They are merely structures, placed in positions within a structure.
If that position becomes available, another A.N.I. will be placed.
The structure of the horde of A.N.I. and the progression of A.N.I.ing should be considered a problem.

## the horde movement
Imagine a society where general-purpose labor robots are widespread. If you provide a robot with the knowledge for a specific task, it will be able to perform it.
Take the construction industry, for example.
By using training data from skilled carpenters, we could translate their expertise into a model that A.I. can use.
This model could then be replicated to mass-produce robots with superhuman carpentry skills and accumulated knowledge.  
But how should we fairly distribute the benefits these robots would bring?
As explained in the previous chapter, concepts are shaped by culture, and individual contributions are often considered negligible. 
Claiming this knowledge as the property of a few amounts to expropriating a collective resource that has been developed over time.
Similar arguments can be made across various fields, including the technologies that enable robots.

The expropriation of historical common property is already occurring at this time.
If you are a creator of painting, music, literature, etc., you are probably feeling hazy about the recent generation of pictures and music by A.I.  
The distribution of profits from technology is determined by existing law and custom without debate, resulting in a biased and fixed distribution.
It is the result of a culture that has not been rewritten due to people's belief that the status quo is *fair* and that economic issues remain unresolved.

A.I.'s model is copyable and will not be reduced by any amount of distribution.
Nonetheless, because of the huge budget required to generate the model, if current practice is followed, the person who financed the model will have ownership of it.
Using a service where the core is a monopoly creates dependence on the monopoly and reinforces a dominance structure.
Think about what it means for a few people to own the wisdom of humanity that has been gradually built up over a very long period.
It is impossible to conduct a project of a public nature based on economic theory.

In the computer world, there is a culture of publishing source code, and the source code to make A.I. work is also available.
This background is due to the free software movement by Richard Stallman in the 1980s.
He believed that source code is the common property of humanity and advocated a license that prevents companies from monopolizing it.
On the other hand, as the business use of computers expanded, licenses that modified this license to make it more easily applicable to commercial use became popular, and the term “open source” came into use.  
Now, open source is a mixture of free software ideology and corporate agendas.
A company will make the source code public when it is more beneficial to share it with many engineers to improve quality than to monopolize it.
It also improves their presence in the engineering community.
In addition, as cloud computing and the use of A.I. expands, value has shifted to data, computational resources, and user enclosures, weakening the reasons for keeping relatively less valuable source code private, so many codes for general-purpose purposes are open source.
Commercial availability is a given for engineers who have internalized social *fairness* based on economic rationality, and the Free Software philosophy is not understood.
Sometimes it is called *democratization*, referring to the ability to do something for no charge, using partially publicly available products that are operated for convenience from the company's point of view.
Engineers are engaged for *everyone* to *do a fair job according to the common sense* dictated by their culture.

In May 2023, Jeffrey Hinton retired from Google.
He has been called the Godfather of A.I. and has driven significant work on A.I. for decades.
He didn't leave because of a problem with Google, but he said that Google has acted very *responsibly*.<span class="footnote">
https://x.com/geoffreyhinton/status/1652993570721210372
</span>
The evolution of AI has been much faster than he envisioned. Therefore he warns that society is not prepared for it. He also calls out risks such as the inevitable use of AI by *villains*, naming names such as Trump and Putin.
Although he has some regrets in part for his past work, he says "I console myself with the normal excuse: If I hadn’t done it, somebody else would have."<span class="footnote">
https://www.nytimes.com/2023/05/01/technology/ai-google-chatbot-engineer-quits-hinton.html</span>
To paraphrase the previous section, if a position becomes available, another A.N.I. will be placed, but the horde movement will never change.

In November 2023, OpenAI's board of directors suddenly decided to remove CEO Sam Altman.
Five days after his dismissal, Altman was reinstated, and several members of the board were replaced at the same time.
At this time, one of the board members who left OpenAI to be replaced, Helen Toner, is a researcher specializing in A.I. risk.  
A report<span class="footnote">Decoding Intensions - Artificial Intelligenceand Costly Signals
https://cset.georgetown.edu/wp-content/uploads/CSET-Decoding-Intentions.pdf
</span> by three co-authors, including Toner, four days before Altman's dismissal triggered the uproar.<span class="footnote">Before Altman’s Ouster, OpenAI’s Board Was Divided and Feuding
https://www.nytimes.com/2023/11/21/technology/openai-altman-board-fight.html</span>
The report accuses OpenAI of a disregard for risk.

> While the system card itself has been well received among researchers interested in understanding GPT-4’s risk profile, it appears to have been less successful as a broader signal of OpenAI’s commitment to safety. The reason for this unintended outcome is that the company took other actions that overshadowed the import of the system card: most notably, the blockbuster release of ChatGPT four months earlier. (...) This result seems strikingly similar to the race-
to-the-bottom dynamics that OpenAI and others have stated that they wish to avoid. OpenAI has also drawn criticism for many other safety and ethics issues related to the launches of ChatGPT and GPT-4, including regarding copyright issues, labor conditions for data annotators, and the susceptibility of their products to “jailbreaks” that allow users to bypass safety controls.

"The System Card”, *which has been well received among researchers*, is a document on A.I. security, submitted with the GPT-4 Technical Report by OpenAI.<span class="footnote">
2024.3.4 GPT-4 Technical Report Appendix H GPT-4 System Card
https://arxiv.org/pdf/2303.08774</span>
The term “race to the bottom” also refers to the overall trend toward the bottom, as competitive pressures lead to a race that disregards safety and ethics rather than competition for product and service quality.
Toner and others point out that OpenAI has failed to present a credible corporate stance, including by accelerating its release, which has put pressure on other companies and caused a race to the bottom.  
Toner et al. emphasize the need for the development and dissemination of A.I. based on a *democratic* process.
However, the ideology did not match the movement of the horde of A.N.I., and it seems that they shook off those who could not follow.

By the way, what does *democratic* mean in an “unresolved economic problem” culture?
Who is *everyone* who can participate in a *democratic* process led by a company?

The “Economic Impact” section of the System Card begins with the following sentence:

> The impact of GPT-4 on the economy and workforce should be a crucial consideration for policymakers and other stakeholders.

This excerpt suggests an intention that it is not within their *responsibility* concerning the economy.
In the previous section, an example about “What is the best bagel place in New York?” was created from the “Economic Impact” statement on the System Card.
It is a manipulative excerpt, but in any case, the only thing it says in this section is “be aware that there will be ‘changes’ in the A.I."
They might leave the economic issues to *everyone* and do their own *responsible* work.
They never see reality.

Furthermore, I would like to point out that the system card does not touch <ins>at all</ins> the reality of the enormous amount of power consumed by computer use.
The huge amount of electricity for AI training and inference is <ins>now being criticized in reality</ins>.
As access to A.I. becomes more integrated into our lives, the more frequently we use computers, the more energy a single person is possible to consume.
If we continue to follow our short-sighted desires and enjoy convenience, it is evident that the earth's resources will run out.  
Otherwise, in the case we limit the total energy available to humanity,
the issue of proper energy distribution remains.
The more energy one person can consume, the more economic inequality will directly translate into energy consumption disparity.
It, in turn, would result in a significant and lasting divide in living conditions.
I predict that, at that point, the vast amounts of energy consumed by the wealthy will primarily be used to maintain their status and positions.
People can distort reality indefinitely, and A.I. is a technology that allows them to do so.

Economic and environmental issues are current problems that are obvious to all, but OpenAI security researchers <ins>naturally</ins><span class="footnote">
The inside of the company follows the dynamics of the company, and the effect of those dynamics is expressed as "naturally"</span> remove them from their view.
They are only engaged in *fair and responsible* work, and they have already lost the freedom to look at reality undistorted.
The risks they are addressing are the military use of A.I.<span class="footnote">
The use of A.I. for military purposes (order maintenance in the view of the regime) is a different kind of threat from the weapons used so far.
First, the program is a threat because it does not always work as intended. Or if it does work as intended, it is a threat because it is easy to target for destruction something inconvenient for those who operate to A.I.</span> and the generation of fakes<span class="footnote">
The primary cause of the problem is social networking and/or online advertising. Increased exposure to fake news and the ease with which self-displaced people can connect has made the problem more pronounced. In other words, it is the result of “recommendations based on user attributes meeting people's needs."
It is fundamental to the current platform's services, and is something they are trying to push even further with A.I.
The A.I. is expected to reduce the cost of fabrication and allow it to spread automatically and on a large scale, and to make targeting more precise, but the security card does not point out any such aspects.
Such aspects are <ins>naturally</ins> out of their sight.
</span>. 
In short, the risk of disrupting <ins>their order</ins>.
  
OpenAI recognized the risk of A.I. from its beginning.
Their idea was that they would lead the development of A.I. and that they would use their power *fairly*.
OpenAI's mission is "to ensure that artificial general intelligence benefits all of humanity."
The implementation of A.I. is being promoted by those who can tolerate the deception that limited, self-centered *everyone* can be referred to as all of humanity.  
What we have found with the 2023 board replacement is that even their flimsy deceptions are driven out of the *answers* of the horde of A.N.I.
Although we have the occasion to get a glimpse of the reality by some of these selves, we do not know what is actually happening inside OpenAI and other companies.
However, it is not difficult to imagine the direction in which the horde is heading.

## Conclusion
The social problems brought about by technology are expected to become more pronounced with A.I., and resisting them comes down to a matter of self and freedom.
For you to be free to use A.I., you must recognize and free yourself from the reality that technology is being used as a tool of domination.

It requires <ins>culture</ins>.
No individual can resist.
We need to acquire **new concepts** that will form the **next order** and share them with people in solidarity.
Social problems are piling up, but they will not be solved by top-down power or top-down technology.
What is needed is a bottom-up solution through a chain of actions, first by you, then by your neighbor, and then by that neighbor's neighbor, and so on.

Many people ask me what the **next order** is or what the **new concept** is.
I have my own ideas, but that is not the answer for you.
You are <ins>free</ins> to think about it.
The first step is to resist A.N.I.ing, which means to act freely, to step away from the central, rational, explicit *answer*, to step into the unknown, and to connect with the periphery.
Then, communicate the acquired concepts to others.
Also receive concepts from others.  
As mentioned in the previous chapter, you form yourself from a collection of concepts from others.
Your ability to self-form depends on the freedom of others.
In this sense, it can be said that “care for others is self-care."

I would be happy to have you discuss the story of this book with your neighbors as part of your care.
Please do not just spread the URL on social networking sites, etc., but make sure to have a face-to-face dialogue with the person you are talking to.
We believe this will create a culture and lead to release from a culture that has not “solved the economic problem”.

Please feel free to contact me at the following address if you have any comments, criticisms, questions, or any other.

{{% definition %}}
email: <a href="mailto:namachi@kuku9.net">namachi@kuku9.net</a>  
mastodon: <a href="https://kolektiva.social/@mzo">@mzo@kolektiva.social </a>
{{% /definition %}}
