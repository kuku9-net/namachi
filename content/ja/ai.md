+++
chapter = 1
title = '人工知能'
+++

本章では、社会的な影響の観点に重きをおいて昨今のAIの流れを概説をします。
できるだけニュースなどで使われる言葉だけを使って、
小難しい話はなしにAIのイメージを伝えるように努めています。

その分、正確さは犠牲にしています。
例えば、

「**Xは**(○と□などに分類されるが、話の筋の上で重要な○ではだいたい)**△が使われている**」

と言おうとすると、
○と□についても説明する羽目になり、本筋と違うところで、どんどん長くなってしまうので、
思い切って括弧の中は省略して

「**Xは△が使われている**」

と言い切ります。
そういう文書だと了承の上、お読みください<span class="footnote">
そもそも「AI」という単語が相当に大味な言葉です
</span>。

# AIの性能を決める三つの因子
{{< figure
place="right"
src="images/neuron.webp"
caption="ネットワークのイメージ(実際は遥かに複雑)"
>}}

人間の脳は膨大なニューロンのネットワークによって機能しています。
ニューロンの間の結合の強さを変化させることで様々な学習と計算が可能になっています。
ニューロンという単純な素子の組み合わせで計算するというアイデアをコンピュータの中に持ち込んで、AIを実現しています。

AIの性能を決める以下の3つの因子があります。

1. ネットワークの構造
2. コンピュータリソース
3. 学習データ

ネットワークの構造は、ニューロンをどれだけ用意して、それらをどのように接続するかによって決定されるものです。
ネットワークの構造によって、根本的にそのAIが何ができるのかが決まります。
研究者たちは日々、より性能の高い構造を探して研究をしています。
新しいネットワークの構造が見つかると、
それに応じて、AIができることやそのために必要なコンピュータリソース、必要な学習データの内容や量が変わります。

ネットワークの構造とコンピュータリソースは、「学ぶための機構」であり、
学習データは「何から学ぶか」になります。
本質的に学習データに含まれないことをAIが学ぶことはできません。

# すこし前のAI
より高度な処理をするためにはより大規模なネットワークが必要で、
ネットワークが大規模になるほど、それを処理するためのコンピュータリソースも増えます。
少し前のAIは、画像認識やレコメンドなど、特定のタスクに特化して作れられていました。
この頃のコンピュータの性能はすでに十分に高かったのですが、
単純に規模を大きくすればAIの賢さが向上するわけではなく、
過剰に大きいネットワークは逆にバカになってしまう問題に突き当りました。
そのためAIの研究テーマは、大規模なネットワークをどのように接続すれば
AIの性能を出すことができるのかが大きな焦点となっていました。

また、実用的なAIを作る上で、学習データを用意することも重要です。
どれだけのデータが必要になるかは学習内容に依存しますが、
AIは学習の要領が悪く、膨大な量のデータを要します。

{{< figure
place="center"
src="images/learning.webp"
caption="AIの学習フェーズ(左)と推論フェーズ(右)"
>}}


AIの学習とは、AIが正しく機能するように膨大な数のニューロンの間の結合の強さを決定することです。
AIは学習データに対して、パターンを見つけ出し、予測を行います。
そして各予測の正確さを確率として表現し、確率的に一番それっぽい答えを探します。
大量の学習データをAIに与えて、AIの答えと教師の答えの違いが十分に小さくなるまで何度も繰り返し、ニューロンの結合を調整します。
学習済みのネットワークを**モデル**と呼びます。
モデルを作るまでが**学習フェーズ**、そのモデルを利用するフェーズのことを**推論フェーズ**といいます。
ここまで、できるだけ専門用語を出さないようにしてきましたが、
「モデル」という言葉と「学習してモデルを作る」イメージだけは理解してください
<span class="footnote">
学習済みのモデルを、与えられた全学習データを圧縮したもの、
つまりデータの冗長性を削ぎ落とし、データが内包する全情報を本質的な表現に変換したものと捉えることもできます。
そのように捉えると推論フェーズは学習した全情報から必要な部分だけを展開しているといえます。
これが分かると見通しがよくなると思いますが、わからなくても本書を読み進める上では問題ありません。
余計混乱するようでしたら、この話は忘れてください。
</span>。
学習フェーズは大量の学習データについて、何度も繰り返し計算するので、計算量も大きくなります。
推論フェーズはひとつの入力に対して一度計算するだけなので、学習フェーズに比べると計算量は小さいです。
また、一度学習済みのモデルが手に入れば、いくらでもコピーして利用することができ、
小規模のモデルならスマートフォン上で動作させることもできます。

# 学習データの重要さ

{{< figure
place="right"
src="images/coco_sample.png"
caption="COCOのデータの例。画像のどの領域が何であるかがラベリングされている"
>}}

学習データの一例として、画像認識を学習するためのデータセットである
COCO(COmmon Objects in Context)<span class="footnote">
https://cocodataset.org/
</span>
を紹介します。
COCOは30万枚以上の画像に加えて、
それぞれの画像中のどの領域に何があるのかをラベリングしたデータが提供されています。
ラベリングの総数は100万件以上になります。
サンプル画像を見ると犬の足元の曲線まで丁寧に仕切られていることが確認できます。
このようなデータは人が一つ一つ手作業で作っています。
この作業を実際にやっているところを想像してみてください。
一枚の画像でも結構大変ではないでしょうか？　
これを数十万枚やるのは大変なコストがかかります<span class="footnote">
AIが大雑把に枠をひいて、それを人間が微調整する技術も進展しています。
それでも実際には作業をちゃんとできているかをチェックする仕事もありますし、
対象が見切れている場合の扱いなど、作業をする人によって差が出ないようにルールを定める仕事もあります。
さらに研究者が使ってみてから見えてくる要件があると、最初からやり直しになることもあります。
</span>。
<ins>AIの実現には、研究者や技術者だけでなく、学習データを作成する人も必要です</ins>。

また学習データに偏りがあると、そこから外れたことに対し正しく回答できないAIができてしまいます。
例えば写真の提供者の属性(国籍、年齢、性別など)が偏っていたら、AIはその偏りが反映された学習をしてしまいます。
どうやって万遍なくデータを収集するかも考えなくてはなりません。
このように「何から学ぶのか」がAIの性能を決める重要な要因になります。

# 生成AI以後
学習データについての困難は生成AIの登場により一変します。  
例として、
「一単語だけ空欄になっている文について、その空欄に適切な単語を生成する」AIを作りたいとします。
このタスクのための学習データは次の方法で手に入ります。
まず、どこかから完成された文を入手します。
そして、そこから一単語を抜き出すと、その抜き出した単語が「答え」となる問題文が手に入ります。
例えば「私は犬と散歩をした」という文から、
「私はXと散歩をした」のXに入る単語を当てる問題と「犬」という答えが作れます。
「犬」が答えと言いましたが、
「お父さん」でもいいし、「猫」と散歩する人もいるかもしれません。
それでも、この学習データについては「犬」が「答え」ということにして学習します。
膨大な学習データの中には「犬」以外の「答え」を含む場合もあるはずで、
その都度調整されていきます。
最終的に学習が完了した時には「私はXと散歩をした」のXに入る単語の発生確率が分かるようになっており、
一番それっぽい単語はどれかが分かるようになります。  
文章を生成する場合は完成された文章を途中から隠せば、問題と答えのセットが作れます。
文章を予測するのは、無理があるように感じますが、
コンピュータは膨大な量を怠けず言われた通りやり続けることができるので、
物量で押し切って学習をやりきることができます。  
このような理由で生成系AIの学習データは
インターネットには膨大なデータをかき集めるだけで用意できるようになりました。  
そのような学習を実現するためには、大規模なサイズのネットワーク構造が必要となります。
それは文脈を把握をしてモデルのパラメータを動的に調整するという
革新的なネットワークの構造がブレークスルーになりました。
この動的な調整によって、ネットワークの規模が大きすぎると逆にバカになる問題が解消し、
モデルを大規模にするほどAIの性能がいくらでも上がっていくようになりました。

AIの性能を決める3つの因子のうち2つの因子が解消し、
あとはコンピュータリソースをどれだけつぎ込むかが肝になりました。
そして実際に膨大なリソースを注ぎ込んだ結果が結実したのがChatGPTです。  
ChatGPTのような高性能なモデルを作ろうとすると、膨大なコンピュータリソースを数週間専有する必要があり、数十億円から数百億円規模のお金がかかります。
モデルのニューロンの接続本数をパラメータ数といいます。
ChatGPTの旧バージョンのモデルであるGPT-3では1750億個のパラメータがあります。
OpenAI社は最新のモデルについてパラメータ数などの詳細を公開していませんが、GPT-3.5では約3000億個のパラメータ、GPT-4は1兆パラメータを超えると噂されており、学習には1億ドル以上のコストがかかったと予想されています。
一回のモデルの計算だけでそれだけかかるので、やり直しが必要になれば、その分のコストがかかります。
そのようなコストを払ってできたChatGPTの性能は皆さんもご存知の通りかと思います
<span class="footnote">
ちなみにOpenAI社は学習フェーズと推論フェーズを合わせたコンピュータリソースの使用料に100億ドルを超える予算を使っており、2024年の業績は大赤字の見込みです
</span>。

もう一点、ChatGPTの特筆すべき特徴として、
「追加で少し教えてあげるだけでその内容を学習できる」ということが知られています
<span class="footnote">
2020/7/22 ["Language Models are Few-Shot Learners"](https://arxiv.org/abs/2005.14165)
</span>。
具体的な応用として、ChatGPTはフェイクニュースや差別的な発言を避けるように、追加の教育を受けています。
従来はインターネットの膨大な文書から適切なものを仕分けたものをAIが学習する必要がありましたが、
ChatGPTは全部ひっくるめて学習してから、その後で何がだめかを教えてあげれば理解できます。  
モデルに追加で学習できることで、特定の目的に適したAIを低いコストで作ることが可能です。

# これからのAI
AIの性能を決める三つの因子にそって、これからのAIについて考えます。

#### 1. ネットワークの構造
現状のAIは確率的にそれっぽいものを生成しているだけなので、
それが論理的に矛盾していても気づかないことがあります。
そのような問題の解消を目指して、
より論理的な思考ができるAIを実現できるよう研究が続いています。
また現在「人間を100点とした時AIは何点か」という視点でAIは評価されがちですが、
AIだからできることにフォーカスしたり、他の技術との親和性が発展したりすることで、
AIの応用範囲が広がっていくことが予測されます。

それとは別の方向性として、効率の改善が重要な課題としてあります。
モデルのサイズを抑制しつつ、高性能なAIを作れるようになると、
AIの作成のコストが下がり、参入障壁が下がります。
また個人のPCやスマートフォンで推論フェーズの実行ができるようになるには
モデルのサイズの大幅な圧縮が必要になります。
金銭的なコストの面でも、エネルギー資源の観点からも、
効率は実用の幅に大きな影響を与えます。

#### 2. コンピュータリソース
現在、膨大なリソースを使うほど性能が上がる状況ですが、まだ天井は見えていないようです。
今後、効率化の方法が見つかれば、リソース負荷が軽減される可能性はありますが、
逆に、より高度なAIのネットワーク構造が見つかることで、さらに膨大なリソースが必要になる可能性もあります。

#### 3. 学習データ
生成AIの台頭により、学習データはとにかく集めればよい状況にあると説明しました。
今後は<ins>学習データの量より質</ins>の時代に移ると予想します。
従来の学習は「みんなができること」をAIでもできるようになることが目標でしたが、
そういうレベルの学習はすでに終わりつつあると感じます。  
これからはより賢いAIのために、質のよい学習データを適切に用意できることが重要になるでしょう。

# まとめ
以降の章を読み進める際に必要な要点をまとめると下記になります。

- AIは学習データから確率的に一番それっぽい答えを導き出す
- 学習データが大事。本質的に学習データに含まれないことをAIは知らないし、学習データが偏っていればAIも偏る
- AIの学習フェーズのコストは高い。学習結果となるモデルを利用する推論フェーズのコストはずっと小さい
- モデルはコピーできる
